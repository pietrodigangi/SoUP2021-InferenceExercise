{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b2590-677a-4be2-92ff-5841ccd9d990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f1151-7409-422f-8dba-aae618bf7c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/// Import useful modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "from scipy.stats import norm \n",
    "from scipy.optimize import minimize\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import quad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a089e3d",
   "metadata": {},
   "source": [
    "# STATISTICAL INFERENCE FOR A DARK MATTER SEARCH\n",
    "You will reproduce a direct detection experiment searching for WIMP dark matter with a dual-phase TPC (a simplified version of XENON1T). \n",
    "\n",
    "Targets of the excercise:\n",
    "\n",
    "- [ ] Build a **2D physical model** for the expected signal and backgrounds \n",
    "- Given a (simulated) dataset, compute the **exclusion limit** on the WIMP cross section using \n",
    "    - [ ] the simple **Likelihood Ratio method**\n",
    "    - [ ] and the **Profile Likelihood Ratio method** introducing nuisance parameters. \n",
    "- [ ] Estimate the **expected experimental sensitivity**\n",
    "- [ ] (Optional extra) Estimate the **expected discovery potential** with 3$\\sigma$ significance and test your dataset for **discovery** (and eventually set a confidence interval on the WIMP cross section).\n",
    "- [ ] (Optional extra) Study the sensitivity as a function of background expected **rate** and **uncertainty**\n",
    "- [ ] (Optional extra) Study the sensitivity as a function of **exposure**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e721572c-dced-40e9-a5da-9ace727367a7",
   "metadata": {},
   "source": [
    "# 1. BUILD THE PHYSICAL MODEL\n",
    "\n",
    "We follow a simplified version of the physical model adopted for the XENON experiments. The oservable signals produced by an interaction are S1 and S2.\n",
    "\n",
    "We can distinguish two types of interactions (electronic recoils, **ERs** and nuclear recoils, **NRs**) which produce very different S2/S1 ratios (much larger for ERs). Therefore one can exploit a parameter based on the S2/S1 ratio to separate and discriminate ERs from NRs. The WIMP signal is expected to be of NR type.\n",
    "\n",
    "We use a 2D analysis space:\n",
    "- **S1 signal** (unit: PE, photoelectrons) | The primary scintillation signal in a TPC. It is a proxy for event energy\n",
    "- **Y discrimination parameter** | The \"flattened\" $\\log_{10}$(S2/S1). A parameter for which electronic (ER) and nuclear recoils (NR) are well separated.\n",
    "\n",
    "For this WIMP search we fix:\n",
    "- The **region of interest** (RoI) for this WIMP search as the interval **(3,70) PE in the S1 space**.\n",
    "- The fiducial target mass as **1000 kg**\n",
    "- The data taking livetime as **2 years**\n",
    "- ... thus the **exposure** is $2\\,t\\,y$ (mass $\\times$ livetime)\n",
    "\n",
    "Signal and background models:\n",
    "- **Signal**: WIMP of 50 GeV/c$^2$ mass \n",
    "- **Backgrounds**\n",
    "    - *ER background*: sum of external (e.g. gammas from detector materials, neutrinos) and intrinsic sources (e.g. Rn222, Kr85) \n",
    "    - *NR background*: sum of neutron (mainly radiogenic neutrons from materials) and neutrino sources (CE$\\nu$NS mainly from solar neutrinos)\n",
    "    \n",
    "We need to model signal and backgrounds in the 2D space (S1, Y). Therefore we need:\n",
    "- Probability density functions (**PDF**s) in S1 and Y\n",
    "- **Expectation values** ($\\mu$), i.e. expected number of events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fdea4c",
   "metadata": {},
   "source": [
    "## 1.1 S1 PDFs\n",
    "We provide you with the expected S1 spectra that you can find in the [GitHub repo](https://github.com/pietrodigangi/SoUP2021-InferenceExercise) of the execrise: \n",
    "- WIMP (50 GeV/c2 mass) signal: `Sig-WIMP-50-S1.txt`\n",
    "- ER background: `Bkg-ER-S1.txt`\n",
    "- NR background: `Bkg-NR-S1.txt`\n",
    "\n",
    "Units: x (S1 signal, PE), y (spectrum, a.u.)\n",
    "\n",
    "Tasks:\n",
    "- [ ] Read input files\n",
    "- [ ] Interpolate the (binned) spectra\n",
    "- [ ] Normalize to PDFs in the RoI (3,70) PE\n",
    "- [ ] Visualize S1 spectra and PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29badc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/// Read input spectra for signal and backgrounds\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# Read signal and background S1 spectra\n",
    "# These are dataframes\n",
    "signal = pd.read_csv('Sig-WIMP-50-S1.txt', sep=' ')\n",
    "backgroundER = #-> your code here <-\n",
    "backgroundNR = #-> your code here <-\n",
    "\n",
    "# Print dataframes and checkout their structure\n",
    "print(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fabc6af-c395-4eb5-bbc8-94e2d92e4b09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/// Interpolate input spectra and plot them\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# Interpolate binned data\n",
    "f_signal = interp1d(signal.S1, signal.spectrum)\n",
    "f_backgroundER = #-> your code here <-\n",
    "f_backgroundNR = #-> your code here <-\n",
    "\n",
    "# Visualize spectra\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "x = np.linspace(3,70,1000)\n",
    "ax.plot(x, f_signal(x)*1e-5, 'b-', color='tab:cyan', label='Signal (WIMP 50 GeV/c2)')\n",
    "#-> your code here <-\n",
    "#-> your code here <-\n",
    "plt.xlabel('S1 [PE]')\n",
    "plt.ylabel('Spectrum [a.u.]')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474d5db2-12f7-4f2a-a936-137bffa9f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/// Signal and background PDFs (S1)\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "def normalize2pdf(func, xmin=3, xmax=70):\n",
    "    '''\n",
    "    Arguments\n",
    "    ---\n",
    "    func: input function or distribution\n",
    "    xmin, xmax: interval of interest\n",
    "    ---\n",
    "    Returns a PDF (distribution normalized to 1)\n",
    "    in the interval (xmin,xmax)\n",
    "    '''\n",
    "    \n",
    "    # compute the integral of original distribution func\n",
    "    #-> your code here <-\n",
    "    \n",
    "    pdf = lambda x : func(x)/integral\n",
    "    return pdf\n",
    "\n",
    "\n",
    "# Tip: quad from scipy.integrate can be used to compute integrals\n",
    "# e.g. integral = quad(f_signal,3,70)\n",
    "pdf_signal = normalize2pdf(f_signal)\n",
    "pdf_backgroundER = #-> your code here <-\n",
    "pdf_backgroundNR = #-> your code here <-\n",
    "\n",
    "\n",
    "# Visualize PDFs\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "x = np.linspace(3,70,1000)\n",
    "ax.plot(x, pdf_signal(x), 'b-', color='tab:cyan', label='Signal (WIMP 50 GeV/c2)')\n",
    "#-> your code here <-\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9d2029-8696-4f0d-97a8-e9f115b3c625",
   "metadata": {},
   "source": [
    "## 1.2 PDFs in discrimination space (Y)\n",
    "\n",
    "To reproduce a typical ER discrimination power of 99.5% (with 50% NR acceptance) in LXe dual-phase TPCs, we assume that ERs and NRs are distributed in an idealized **discrimination parameter Y** as *Gaussian* with the following mean and standard deviation:\n",
    "- ER: `mean=0`, `std=1`\n",
    "- NR: `mean=-2.58`, `std=0.92`\n",
    "\n",
    "Our model components are of type:\n",
    "- WIMP signal is expected to be of **NR** type\n",
    "- **ER** background\n",
    "- **NR** background\n",
    "\n",
    "\n",
    "Tasks:\n",
    "- [ ] Define the two Gaussian PDFs for ER and NR (that will be properly associated to signal (NR) and backgrounds as PDFs in the Y space).\n",
    "- [ ] Visualize such PDFs in the Y space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3b2192-a436-4819-aba8-295d41f2d8cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/// Discrimination space (Y)\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# ER and NR distributions\n",
    "# This reproduces the following separation between ER and NR populations:\n",
    "# 99.5% ER rejection with 50% NR acceptance\n",
    "# Tip: exploit norm from scipy.stats\n",
    "nr_discr = norm(loc=-2.58, scale=0.92)\n",
    "er_discr = #-> your code here <-\n",
    "\n",
    "# Visualize PDFs\n",
    "#-> your code here <-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d0a77c-508e-42ed-adbf-612618f573f4",
   "metadata": {},
   "source": [
    "## 1.3 Rates and expectation values\n",
    "\n",
    "We assume the following rates for signal and backgrounds in the S1 RoI (3, 70) PE:\n",
    "- WIMP signal: `2.68 events/t/y` (corresponding to a WIMP cross section = $1.0\\times10^{-47}$ cm$^2$)\n",
    "- ER background: `61.98 events/t/y`\n",
    "- NR background: `2.01 events/t/y`\n",
    "\n",
    "Note #1: We are using here rates vaguely similar to those expected in the XENON1T experiment in the core 1 t fiducial volume. One usually estimates and predict the signal and backgrounds in the recoil energy space (keV); then you need to apply the detector response model (to ERs and NRs) to convert recoil energy into observable S1 (and S2) signals. (This goes beyond the scope of this excercise, therefore we directly work in the S1 space, with the spectra provided in section 1.1)\n",
    "\n",
    "Note #2: WIMP rate is based on standard DM halo assumptions and WIMP-nucleon spin-independent coupling for a LXe target. You can easily convert number of signal events $n_{sig}$ into **WIMP cross section**: $\\sigma=\\frac{n_{sig}}{rate_{sig}\\times exposure} \\times 10^{-47} \\ cm^2$\n",
    "\n",
    "Note #3: The ER background rate is reduced compared to XENON1T (just to make the following computations less intensive and time consuming)\n",
    "\n",
    "Task:\n",
    "- [ ] Calculate the **expectation values** (no. of expected events) in the fixed exposure of 2 ty: $\\mu_{sig}$ (signal), $\\mu_{bkgER}$ (ER background), $\\mu_{bkgNR}$ (NR background) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d25abb-f41d-4cb3-bd02-82e96ae38213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/// Rates and expectation values\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# Rates \n",
    "# Expected number of events in the S1 ROI \n",
    "# per unit energy (or better S1 signal amplitude)\n",
    "# per unit target mass \n",
    "# per unit time\n",
    "rate_sig   = 2.68 #(/t/y) # 50 GeV/c2 WIMP with cross section = 1.0e-47 cm2\n",
    "#-> your code here <-\n",
    "#-> your code here <-\n",
    "\n",
    "# Exposure\n",
    "#-> your code here <-\n",
    "\n",
    "\n",
    "def nsig2xs(nsig, exp = exposure):\n",
    "    '''\n",
    "    Arguments\n",
    "    ---\n",
    "    nsig: number of signal events\n",
    "    exp: exposure\n",
    "    ---\n",
    "    Converts no. of signal events into cross section\n",
    "    given a benchmark rate of 0.04 events/PE/t/y\n",
    "    for a cross section of 1.0e-47 cm2\n",
    "    \n",
    "    Returns a cross section scaling factor\n",
    "    with respect to the benchmark 1.0e-47 cm2\n",
    "    '''\n",
    "    #-> your code here <-\n",
    "    \n",
    "\n",
    "#/// Expectation values\n",
    "mu_sig   = rate_sig * exposure\n",
    "mu_bkgER = #-> your code here <-\n",
    "mu_bkgNR = #-> your code here <-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5abdb30-38bf-4c30-9c20-5f6451f29c9e",
   "metadata": {},
   "source": [
    "## 1.4 Toy-Dataset random generator\n",
    "You can simulate \"observed\" data based on your statistical model. A simulated dataset is usually called toy-MC, toy-dataset or pseudo-experiment.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "- [ ] Write a routine to generate random datasets in the 2D space (S1,Y) based on the signal/background PDFs and expectation values. For each model component you need:\n",
    "    - Poissonian random generation of **number of events** (given the expectation value $\\mu$)\n",
    "    - For each event, random sampling of **S1** and **Y** (given the S1 and Y PDFs) \n",
    "- [ ] Try out one toy-dataset generation, visualize events in the 2D space, compare the outcome with the expectation values and distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a2aebe-9c4a-45ac-93b7-d529ca597ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/// Toy Dataset Generator (in 2D space: S1,Y)\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# Custom defined routine to random sample values from a PDF\n",
    "def random_sampling_pdf(x0,x1,pdf,size):\n",
    "    '''\n",
    "    Random sampling based on custom PDF\n",
    "    x0   : x lower bound\n",
    "    x1   : x upper bound\n",
    "    pdf  : custom function normalized to 1\n",
    "    size : number of events to generate\n",
    "    '''\n",
    "    sample=[]\n",
    "    nLoop=0\n",
    "    while len(sample)<size:\n",
    "        x=np.random.uniform(low=x0,high=x1)\n",
    "        prob=pdf(x)\n",
    "        assert prob>=0 and prob<=1\n",
    "        if np.random.uniform(low=0,high=1) <= prob:\n",
    "            sample.append(x)\n",
    "        nLoop+=1 \n",
    "    return sample\n",
    "\n",
    "# \n",
    "def generate_subset(S1_pdf, Y_pdf, size):\n",
    "    '''\n",
    "    Returns a Pandas dataframe with random generated events\n",
    "    S1_pdf : pdf_signal, pdf_backgroundER or pdf_backgroundNR\n",
    "    Y_pdf  : er_discr or nr_discr\n",
    "    size   : number of events\n",
    "    '''\n",
    "    S1s = #-> your code here <- # (array of S1 values)\n",
    "    Ys  = Y_pdf.rvs(size=size)  # (array of Y values) random sampling from scipy.stats objects (norm)\n",
    "    dict = {'S1': S1s, 'Y': Ys} # store data arrays in a dictionary\n",
    "    df = pd.DataFrame(dict)     # define a dataframe and fill it with the data dictionary\n",
    "    return df\n",
    "\n",
    "\n",
    "#/// Quick example ////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/// Number of events (Poisson generation)\n",
    "n_sig   = np.random.poisson(mu_sig * 10) # enhance no. of signal events (just to visualize it better) \n",
    "n_bkgER = #-> your code here <-\n",
    "n_bkgNR = #-> your code here <-\n",
    "\n",
    "#/// Dataset generation\n",
    "dataset_sig   = generate_subset(pdf_signal,nr_discr,n_sig)\n",
    "dataset_bkgER = #-> your code here <-\n",
    "dataset_bkgNR = #-> your code here <-\n",
    "dataset = pd.DataFrame()\n",
    "dataset = dataset.append([dataset_sig, dataset_bkgER, dataset_bkgNR]) # toy experiment\n",
    "\n",
    "# Plot dataset\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot()\n",
    "# Draw events as scatter points in the (S1, Y) space\n",
    "ax.scatter(dataset_bkgER.S1, dataset_bkgER.Y, color='r', alpha=0.35, label='Background ER (%d events)' % n_bkgER)\n",
    "#-> your code here <-\n",
    "#-> your code here <-\n",
    "# draw Y distributions (median and 2 sigma band)\n",
    "#-> your code here <-\n",
    "\n",
    "plt.xlabel('S1 [PE]')\n",
    "plt.ylabel('Discrimination Parameter Y')\n",
    "plt.legend() \n",
    "plt.show()\n",
    "#/// Quick example ////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "\n",
    "def plot_toy_dataset(subsets, mu_sig=None):\n",
    "    '''\n",
    "    Arguments\n",
    "    ---\n",
    "    subsets: array of datasets of the three components (signal, bkgER, bkgNR)\n",
    "    ---\n",
    "    Plot a toy dataset in 2D space (S1, Y)\n",
    "    '''\n",
    "    dataset_sig = subsets[0]\n",
    "    dataset_bkgER = subsets[1]\n",
    "    dataset_bkgNR = subsets[2]\n",
    "    n_sig = len(dataset_sig)\n",
    "    n_bkgER = len(dataset_bkgER)\n",
    "    n_bkgNR = len(dataset_bkgNR)\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    #-> your code here <-\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def generate_toy_dataset(mu_sig, mu_bkgER=mu_bkgER, mu_bkgNR=mu_bkgNR, plot=False):\n",
    "    '''\n",
    "    Returns\n",
    "    dataset : a dataframe of generated events (Toy experiment)\n",
    "    subsets : list of dataframes for signal, background ER and NR\n",
    "    '''\n",
    "    #-> your code here <-\n",
    "    subsets = [dataset_sig, dataset_bkgER, dataset_bkgNR]\n",
    "    dataset = pd.DataFrame()\n",
    "    dataset = dataset.append(subsets) # toy experiment\n",
    "    \n",
    "    if plot:\n",
    "        plot_toy_dataset(subsets, mu_sig)\n",
    "    \n",
    "    return dataset, subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e4352-e518-49e1-a8e9-64f7940ea45e",
   "metadata": {},
   "source": [
    "# 2. STATISTICAL INFERENCE WITH THE LIKELIHOOD RATIO METHOD (Exclusion limit)\n",
    "\n",
    "Possible hypotheses at play:\n",
    "- Signal hypotheses $H_\\mu$ (as a function of the *signal strength* $\\mu_{sig}$): signal exists with an expectation value $\\mu = \\mu_{sig}$.\n",
    "- Null (background-only) hypothesis $H_0$: signal does not exists, i.e. $\\mu=0$.\n",
    "\n",
    "**Parameter of interest** (PoI): WIMP cross section $\\sigma$ (or equivalently the signal strength $\\mu_{sig}$, just a linear transformation between them.)\n",
    "\n",
    "**Inference for signal exclusion:** you want to test signal hypotheses $H_\\mu$ and evaluate the significance of their *rejection* based on the observed data.\n",
    "\n",
    "**Exclusion limit at 90% CL:** The WIMP cross section (or signal strength $\\mu_{sig}$) at which the signal hypothesis ($H_{\\mu}$) is rejected with 90% CL, i.e. the probability to get a larger disagreement between the observed data and the signal hypothesis (observed p-value) is 1-CL = 0.1. \n",
    "\n",
    "To evaluate how well (or not) your physical model fits the observed data you need to:\n",
    "- define the **Likelihood function** $L(\\mu_{sig})$\n",
    "- fit data (via log-likelihood minimization) and find the **maximum likelihood estimator** (MLE) of the PoI: $\\hat{\\mu}_{sig}$\n",
    "\n",
    "To compute the significance (i.e. the **p-value**) of the rejection of a given signal hypothesis $H_{\\mu}$, we use the **Likelihood Ratio test statistic** ($q_{\\mu}$) and we need to know what is the expected distribution (**PDF**) of the test statistic under such signal hypothesis $f(q_{\\mu}|H_\\mu)$.\n",
    "\n",
    "For searches of rare events, one needs to estimate $f(q_{\\mu}|H_\\mu)$ for each $\\mu_{sig}$ to be tested via toy-MC: random generation of many pseudo-experiments (toy dataset) and evaluate $q_{\\mu}$ on each of them. The required statistic is $>10^{3-4}$ toy-MCs. This is quite computationally expensive, therefore we provide you with already estimated $f(q_{\\mu}|H_\\mu)$ for $\\mu_{sig}$ for the following signal strengths: `mu_set = [0.1,1,5,9,13,15,19]`.\n",
    "\n",
    "\n",
    "Task: \n",
    "- [ ] Given a dataset, perform the statistical inference using the **Likelihood Ratio method** to derive the exclusion limit on the WIMP cross section at 90% confidence level (CL). For that you need to:\n",
    "    - [ ] Fix the dataset: Read the already produced dataset available in the excercise repo: `example_dataset_0.pkl` or generate a toy-dataset (imposing $\\mu_{sig}=0$)\n",
    "    - [ ] Write the likelihood function $L(\\mu_{sig})$\n",
    "    - [ ] Fit your dataset, find $\\hat{\\mu}_{sig}$, plot the log-likelihood curve vs $\\mu_{sig}$\n",
    "    - [ ] Write the Likelihood Ratio test statistic ($q_{\\mu}$)\n",
    "    - [ ] Compute the value of the test statistic $q_{\\mu}$ for the given dataset for different signal strengths $\\mu_{sig}$ (in `mu_set`) and\n",
    "    - [ ] compute the associated **p-value** based on the test statistic PDF under the signal hypothesis $f(q_{\\mu}|H_\\mu)$\n",
    "    - [ ] Find $\\mu_{sig}$ at which p-value = 0.1 (this is your limit at 90% CL!)\n",
    "    - [ ] Convert the limit from number of events into cross section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df7b70b-0e55-4c0f-a938-071e92a6c2c5",
   "metadata": {},
   "source": [
    "## 2.1 Likelihood function\n",
    "\n",
    "We use the *extended unbinned likelihood function*:\n",
    "\\begin{equation} \\label{eq:likelihood_LR}\n",
    "L(\\mu_{sig}) = \\frac{\\mu_{tot}^{N_{obs}}}{N_{obs}!} \\, e^{-\\mu_{tot}} \\, \\prod^{N_{obs}}_{i=1}P(S1_i,Y_i) = \\frac{e^{-\\mu_{tot}}}{N_{obs}!} \\,   \\prod^{N_{obs}}_{i=1} \\mu_{tot} \\, P(S1_i,Y_i) \\: ,\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "- $N_{obs}$ is the total number of observed events, \n",
    "- $\\mu_{tot} = \\mu_{sig} + \\mu_{bkgER} + \\mu_{bkgNR}$, \n",
    "\n",
    "and\n",
    "\n",
    "\\begin{split}\n",
    "P(S1_i,Y_i) = & \\, \\frac{1}{\\mu_{tot}}\\big[\\mu_{sig} \\, f_{sig}(S1_i) \\, g_{NR}(Y_i) + \\mu_{bkgER} \\, f_{bkgER}(S1_i) \\, g_{ER}(Y_i) + \\mu_{bkgNR} \\, f_{bkgNR}(S1_i) \\, g_{NR}(Y_i)\\big] \\: ,         \n",
    "\\end{split}\n",
    "\n",
    "with $f_{x}$ is the S1 PDF of the model component $x$ (sig, bkgER or bkgNR), $g_{k}$ is the Y PDF ($k$ stands for ER or NR).\n",
    "\n",
    "\n",
    "**It is useful to work with $-2\\log L$, which for our statistical model takes the form:**\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "-2 \\, \\log \\,  L(\\mu_{sig}) & = 2 \\, \\mu_{tot} -2 \\, \\sum_{i=1}^{N_{obs}} \\log \\big[\\mu_{tot} \\, P(S1_i,Y_i) \\big] \\\\\n",
    "                             & = 2 \\, (\\mu_{sig}+\\mu_{bkgER}+\\mu_{bkgNR}) -2 \\, \\sum_{i=1}^{N_{obs}} \\log \\big[\\mu_{sig} \\, f_{sig}(S1_i) \\, g_{NR}(Y_i) \\\\\n",
    "                             & \\quad +\\mu_{bkgER} \\, f_{bkgER}(S1_i) \\, g_{ER}(Y_i) + \\mu_{bkgNR} \\, f_{bkgNR}(S1_i) \\, g_{NR}(Y_i)\\big] \\: .\n",
    "\\end{split}\n",
    "\\label{eq:log-l_LR}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60efacd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/// Likelihood function definition\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "def logLikelihood(parameter):\n",
    "    '''\n",
    "    Returns -2 * Log-Likelihood function\n",
    "    '''\n",
    "    # expectation values\n",
    "    mu_sig = parameter\n",
    "    mu_tot = #-> your code here <-\n",
    "    # data points\n",
    "    # Tip: exploit a global variable \"dataset\", which this funtion will automatically refer to\n",
    "    x = np.array(dataset.S1)\n",
    "    y = np.array(dataset.Y)\n",
    "    # log-Likelihood\n",
    "    LL = - mu_tot\n",
    "    for i in range(0, len(dataset)):\n",
    "        Pi = #-> your code here <-\n",
    "        LL += np.log(Pi)\n",
    "    return -2*LL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47771912",
   "metadata": {},
   "source": [
    "### 2.1.1 Likelihood curve vs $\\mu_{sig}$ and the MLE $\\hat{\\mu}$\n",
    "For a given dataset:\n",
    "- perform a likelihood fit of data (minimize $-2\\log L(\\mu_{sig})$), i.e. find the maximum likelihood estimator $\\hat{\\mu_{sig}}$, \n",
    "- plot $-2\\log L(\\mu_{sig})$ for a resonable range of $\\mu_{sig}$ (e.g. (-10,20)) and \n",
    "- check that $-2\\log L(\\hat{\\mu}_{sig})$ is its minimum value (or conversely $L(\\hat{\\mu}_{sig})$ is the *maximum likelihood*)\n",
    "\n",
    "Try with a randomly generated toy-dataset by yourself or use the **example dataset** available in the excercise repository: it is a dataframe stored in the file `example_dataset_0.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a0b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/// Read the dataset (read a dataframe stored in a pickle file)\n",
    "dataset = pd.read_pickle('example_dataset_0.pkl')\n",
    "# Print the dataset dataframe\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f913208-712b-438d-806f-20c6aecd60b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/// Likelihood fit on an example toy-experiment dataset\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# Fix signal hypothesis mu_sig\n",
    "#-> your code here <-\n",
    "\n",
    "#/// Generate a toy-dataset\n",
    "#-> your code here <-\n",
    "\n",
    "#/// Unconstrained likelihood minimization\n",
    "#/// Maximum likelihood estimator of mu_sig\n",
    "# Tip: Use minimize from scipy.optimize\n",
    "# Usage: minimize(function, initial value of parameter/s, paramter/s boundary)\n",
    "# Use sensible intial parameter (mu_sig) value and boundary \n",
    "# to help the minimizer to converge \n",
    "fit_res = minimize(logLikelihood, mu_sig, bounds=[(-100,100)])\n",
    "mle = fit_res['x'][0] # fit_res['x'] is the array of best-fit parameters\n",
    "print('Fit convergence:', fit_res['success'])\n",
    "print(r'Maximum Likelihood Estimator (MLE) µ_sig-hat:', mle)\n",
    "print(fit_res)\n",
    "\n",
    "#/// Plot log-Likelihood function vs mu_sig\n",
    "mus = np.linspace(0,mle*2,50) # array with a range of mu_sig values\n",
    "LL = [logLikelihood(mu) for mu in mus] # array with log-likelihood(mu_sig) value\n",
    "#-> your code here <-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e72f8-536d-4be7-a133-f78f895326c3",
   "metadata": {},
   "source": [
    "## 2.2 Test statistic: Likelihood ratio $q_\\mu$\n",
    "\n",
    "The likelihood ratio test statistic for signal exclusion $q_\\mu$ is defined as:\n",
    "\n",
    "\\begin{equation} \\label{eq:test_stat_cases}\n",
    "q_\\mu =\n",
    "\\begin{cases}\n",
    "-2 \\, \\log \\frac{L(\\mu_{sig})}{L(\\hat{\\mu}_{sig})} & \\text{if } \\hat{\\mu}_{sig} \\leq \\mu_{sig} \\\\\n",
    "0                                               & \\text{if } \\hat{\\mu}_{sig} > \\mu_{sig} \\: .\n",
    "\\end{cases}\n",
    "\\end{equation}  \n",
    "\n",
    "Increasing values of $q_\\mu$ indicate larger discrepancy between the tested signal hypothesis, i.e. signal strength $\\mu_{sig}$, and the observed data (which show the best agreement with $\\hat{\\mu}_{sig}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b037cd4-673c-43dd-82da-7969f0d25a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/// Likelihood ratio test statistic (for signal hypotesis rejection)\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "def q_mu(mu_sig, mle):\n",
    "    '''\n",
    "    Arguments\n",
    "    ---\n",
    "    mu_sig : signal strength under test\n",
    "    mle    : maximum likelihood estimator of mu_sig from likelihood fit on the observed dataset\n",
    "    \n",
    "    Returns the likelihood ratio test statistic value\n",
    "    '''\n",
    "    #-> your code here <-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453d7ca8-8358-4dff-b664-b966b19de1c7",
   "metadata": {},
   "source": [
    "### 2.2.1 Test statistic PDF under signal hypotheses: $f(q_\\mu|H_\\mu)$\n",
    "\n",
    "You can generate O(1k) or more toy-datasets under given signal strenghts $\\mu_{sig}$ and collect the values of $q_\\mu$ for each dataset and build the PDF $f(q_\\mu|H_\\mu)$.\n",
    "\n",
    "Sensible range of $\\mu_{sig}$ is about (1, 20), for our physical model. The generation of 1 toy-MC and likelihood minimization can take ~1 or few seconds (you can check by yourself, and maybe try speed this up). Hence the generation of O(1k) times several $\\mu_{sig}$ hypotheses can take several hours.\n",
    "\n",
    "You can retrieve already produced test statistic distributions $f(q_\\mu|H_\\mu)$ from the pickle file `test_stat_distributions.pkl` for some $\\mu_{sig}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b89d1cd-10af-4d07-912a-da545af94cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/// File with test statistic distributions\n",
    "picklefile = 'test_stat_distributions.pkl' # file storing test statistic distributions\n",
    "\n",
    "# Read test statistic dataframes from file\n",
    "df = pd.read_pickle(picklefile)\n",
    "# Print the dataframe\n",
    "print(df.keys())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb0c0d4",
   "metadata": {},
   "source": [
    "Write a routine to compute $f(q_\\mu|H_\\mu)$ by yourself, if you want to try out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a88bb-8ea4-41ee-a825-125b30c16777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/// Test statistic distribution under a given signal hypothesis: mu_sig\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "def qmu_Hmu_distr(mu_sig, ntoys=1000, write2pickle=True):\n",
    "    '''\n",
    "    Compute f(q_mu|H_mu) with toy-MC (many toy-datasets)\n",
    "    Test static distribution under signal hypothesis H_mu\n",
    "    For a given signal strength mu_sig tested\n",
    "    \n",
    "    mu_sig : test signal strength\n",
    "    ntoys  : no. of toy-experiment datasets to be simulated\n",
    "    \n",
    "    write2pickle : save q values in a dataframe and write to pickle file\n",
    "    \n",
    "    Returns array of test statistic values\n",
    "    '''\n",
    "    print('Generating %d toy-datasets...' % ntoys)\n",
    "    out = widgets.HTML()\n",
    "    display(out)\n",
    "    \n",
    "    q_Hmu = [] # array of test statistic values for all toy-datasets\n",
    "    #/// Loop over toy-dataset generations\n",
    "    for i in range(0,ntoys):\n",
    "        # toy-dataset generation with signal (mu_sig)\n",
    "        # Tip: define a global varable \"dataset\" and assign to it the generated dataset\n",
    "        # so that the likelihood function will refer to it\n",
    "        global dataset\n",
    "        dataset, subsets = generate_toy_dataset(mu_sig = mu_sig, plot=False)\n",
    "        \n",
    "        # maximum likelihood estimator for mu_sig\n",
    "        #-> your code here <-\n",
    "        mle = #-> your code here <-\n",
    "        \n",
    "        # test statistic value for the specific toy-dataset\n",
    "        q = #-> your code here <-\n",
    "        q_Hmu.append(q) \n",
    "        \n",
    "        # print progress information\n",
    "        string = \"<p>Progress: %d / %d</p>\" % (i+1, ntoys)\n",
    "        out.value = string\n",
    "    \n",
    "    if write2pickle:\n",
    "        # Read a dataframe from pickle file\n",
    "        df_q = pd.read_pickle(picklefile)\n",
    "        # define a column name (depending on mu_sig)\n",
    "        column = 'q_Hmu_%.1f' % mu_sig\n",
    "        # Fill a dataframe entry with the array of q values\n",
    "        df_q[column] = [q_Hmu]\n",
    "        # write the dataframe to pickle file\n",
    "        df_q.to_pickle(picklefile)\n",
    "        print('Array %s written to file %s' % (column, picklefile))\n",
    "        \n",
    "    return q_Hmu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c060aab",
   "metadata": {},
   "source": [
    "## 2.3 p-value calculation\n",
    "\n",
    "The p-value of a given dataset realization for the rejection of a signal hypothesis $\\mu$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "p_\\mu = \\int_{q_\\mu^{obs}}^{\\infty} f(q_\\mu|H_\\mu) \\, \\text{d}q_\\mu \\: ,\n",
    "\\end{equation}\n",
    "\n",
    "where $q_\\mu^{obs}$ is the test statistic value (testing the signal strength $\\mu$) calculated on the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4b621e-2f30-4bdf-bc40-b57534a50eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/// Compute p-value\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "def get_pvalue(test_stat_distr, q):\n",
    "    '''\n",
    "    Returns p-value\n",
    "    ---\n",
    "    test_stat_distr : (array) test statistic distribution (e.g. f(qmu|Hmu) for Hmu rejection)\n",
    "    q : a specific test statistic value\n",
    "    '''\n",
    "    #-> your code here <-\n",
    "    return pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71bcc5",
   "metadata": {},
   "source": [
    "Checkout the test statistic distribution for a given $\\mu_{sig}$ and compute the p-value for a given $q_{\\mu}$ value..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa3857-738e-4cfa-a8c7-53988157e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/// Plot f(qmu|Hmu) \n",
    "#///(optional) show a given q value and the corresponding p-value) \n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "def plot_test_stat_distr_Hmu(mu_sig, nbins=30, xmax=None,  q=None):\n",
    "    '''\n",
    "    Arguments\n",
    "    ---\n",
    "    mu_sig : signal strength under test\n",
    "    nbins  : number of bins for histogram plotting \n",
    "    xmax   : max value of x-scale (q value)  \n",
    "    q      : a specific test statistic value\n",
    "    '''\n",
    "    #/// Retrieve f(qmu|Hmu) for mu=mu_sig\n",
    "    df = #-> your code here <-\n",
    "    # (array) test statistic distribution f(qmu|Hmu)\n",
    "    q_Hmu = #-> your code here <-\n",
    "    \n",
    "    # Set x right bound\n",
    "    if xmax!=None:\n",
    "        max_x = xmax\n",
    "    else:\n",
    "        max_x = max(q_Hmu) # max value of test statistic distribution\n",
    "        \n",
    "    #/// Plot test statistic distribution\n",
    "    # Tip: Use plt.hist with density=True to normalize the histogram to a PDF\n",
    "    plt.hist(q_Hmu, range=(0,max_x), bins=nbins, density=True, \n",
    "             histtype='stepfilled', color='gainsboro', edgecolor='b', alpha = 0.5, lw=3,\n",
    "             label='$f(q_\\mu|H_\\mu)$')\n",
    "    \n",
    "    # Show test statistic outcome and p=value\n",
    "    if q != None:\n",
    "        pvalue = get_pvalue(q_Hmu, q)\n",
    "        plt.axvline(q, color='r', label='p-value = %f' % pvalue) # show a vertical line\n",
    "    \n",
    "    #-> your code here <-\n",
    "    plt.title('Test $\\mu_{sig}=%.1f$ - q value=%.2f' % (mu_sig,q))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b87d09a",
   "metadata": {},
   "source": [
    "## 2.4 Exclusion limit at 90% CL\n",
    "\n",
    "For the dataset used previously, compute the exclusion p-value for signal hypotheses approximately in the range $\\mu_{sig}$ = (1, 20) and find the value of $\\mu_{sig}$ for which p-value = 0.1 (90% CL): $\\mu_{sig}^{limit}$. \n",
    "\n",
    "Convert $\\mu_{sig}^{limit}$ into WIMP cross section... that's your 90% CL exclusion limit on the WIMP-nucleon spin-independent coupling for WIMPs of mass 50 GeV/c2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b701fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/// Compute exclusion limit for a single toy-experiment\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "def exclusion_limit(data, cl=0.90):\n",
    "    '''\n",
    "    Arguments\n",
    "    ---\n",
    "    data : dataset to analyze\n",
    "    cl   : confidence level of exclusion limit\n",
    "    \n",
    "    Routine to calculate the exclusion limit based on a given dataset\n",
    "    '''\n",
    "\n",
    "    #/// Dataset to analyze \n",
    "    # (instantiate as global variable, used by the likelihood function)\n",
    "    #-> your code here <-\n",
    "    \n",
    "    #/// Plot dataset in (S1,Y) space\n",
    "    #-> your code here <-\n",
    "    \n",
    "    #/// Likelihood fit: find MLE of mu_sig\n",
    "    #-> your code here <-\n",
    "    \n",
    "    #/// Plot log-Likelihood function vs mu_sig\n",
    "    #-> your code here <-\n",
    "\n",
    "    #/// Read test statistic distributions f(qmu|Hmu)\n",
    "    #-> your code here <-\n",
    "    \n",
    "    # array of tested mu_sig \n",
    "    # (corresponding to the available test statistic distributions\n",
    "    # in the dataframe in \"test_stat_distributions.pkl\")\n",
    "    mu_test = [0.1,1,5,9,13,15,19] \n",
    "    \n",
    "    #/// Compute p-value for H_mu rejection for different signal strengths mu_sig\n",
    "    pvalues = [] # array of p-values\n",
    "    for mu in mu_test:\n",
    "        # retrieve test statistic distribution\n",
    "        #-> your code here <-\n",
    "        \n",
    "        # compute q\n",
    "        #-> your code here <-\n",
    "        \n",
    "        # compute p-value\n",
    "        p = #-> your code here <-\n",
    "        pvalues.append(p) \n",
    "        \n",
    "        # plot f(qmu|Hmu) and q\n",
    "        #-> your code here <-\n",
    "\n",
    "    #/// Plot p-value vs mu\n",
    "    #-> your code here <-\n",
    "\n",
    "    #/// Compute exclusion limit a 90% CL\n",
    "    p_limit = 1-cl\n",
    "    f_inv = interp1d(pvalues, mu_test) # interpolate the inverse function (as you look for the mu value corresponding to p=0.1)\n",
    "    limit = #-> your code here <-\n",
    "    # show limit value (in terms of both mu_sig and cross section)\n",
    "    #-> your code here <-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c444a087",
   "metadata": {},
   "source": [
    "# 3. INFERENCE WITH THE PROFILE LIKELIHOOD RATIO (Exclusion limit)\n",
    "\n",
    "Now we introduce **systematic uncertainty** on the expected background rates:\n",
    "- $\\sigma_{bkgER}$ = 40% | uncertainty on ER background\n",
    "- $\\sigma_{bkgNR}$ = 50% | uncertainty on NR background\n",
    "\n",
    "To take uncertainties into account, we need to include **nuisance parameters** $(t_{bkgER}, t_{bkgNR})$ in the likelihood function $L(\\mu_{sig};t_{bkgER}, t_{bkgNR})$ that rescale the background expectation values as\n",
    "- $\\mu_{bkgER}(t_{bkgER}) = \\mu_{bkgER} * (1 + t_{bkgER} * \\sigma_{bkgER})$\n",
    "- $\\mu_{bkgNR}(t_{bkgNR}) = \\mu_{bkgNR} * (1 + t_{bkgNR} * \\sigma_{bkgNR})$\n",
    "\n",
    "\n",
    "\n",
    "We now use the **Profile Likelihood Ratio** as **test statistic** for the rejection of signal hypotheses.\n",
    "\n",
    "\\begin{equation}\n",
    "q_\\mu=\n",
    "\\begin{cases}\n",
    "-2 \\, \\log \\, \\frac{L(\\mu , \\, \\hat{\\hat{\\boldsymbol{\\theta}}}(\\mu))}{L(\\hat{\\mu} , \\, \\hat{\\boldsymbol{\\theta}})} & \\hat{\\mu} \\leq \\mu\\\\[1.5ex]\n",
    "0 & \\hat{\\mu} > \\mu \\: ,\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\boldsymbol{\\theta} = (t_{bkgER}, t_{bkgNR})$ and $\\mu$ simply stands for $\\mu_{sig}$.\n",
    "\n",
    "This requires two different likelihood fits on data:\n",
    "- **Unconditional fit** | where we find the MLE of the parameter of interest and nuisance parameters that maximize the likelihood function, i.e. $L(\\hat\\mu, \\hat{t}_{bkgER}, \\hat{t}_{bkgNR})$\n",
    "- **Conditional fit** | where $\\mu_{sig}$ is fixed at the signal strength under test, thus one computes the conditional maximum likelihood $L(\\mu; \\hat{\\hat{t}}_{bkgER}, \\hat{\\hat{t}}_{bkgNR})$\n",
    "\n",
    "Tasks:\n",
    "- [ ] Write the Likelihood function (now depending also on $t_{bkgER}$ and $t_{bkgNR}$)\n",
    "- [ ] Write the Profile Likelihood Ratio test statistic\n",
    "- [ ] With the same dataset, compute the observed test statistic value,\n",
    "- [ ] compute the p-value for different tested signal strengths $\\mu_{sig}$\n",
    "- [ ] Find the limit at 90% CL\n",
    "- [ ] Compare with the exclusion limit found with the simple Likelihood Ratio method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8257b3-d650-4673-a789-a249ba628bbf",
   "metadata": {},
   "source": [
    "## 3.1 Nuisance parameters\n",
    "\n",
    "We translate our systematic uncertainty on background rates by introducing nuisance parameters and assuming *Gaussian constraints* for them. Hence they follow a normal distribution. The $\\sigma_{bkg}$ values correspond to a 1 sigma deviation from the nominal expectation value $\\mu_{bkg}(t=0)$. \n",
    "\n",
    "Tip: fix a lower bound to the nuisance parameter values in order to avoid unphysical expectation values (<0). E.g. $t_{bkgER}^{min} = -1 / \\sigma_{bkgER}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82282d4-1875-4e92-a133-b258bc0c731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/// Nuisance parameters:\n",
    "#/// t_bkgER affecting the ER background rate\n",
    "#/// t_bkgNR affecting the NR background rate\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# Define useful parameters related to nuisance parameters\n",
    "sigma_bkgER = #-> your code here <-\n",
    "#-> your code here <-\n",
    "\n",
    "# Avoid unphysical negative rates -> lower bound to nuisance parameters\n",
    "lowerbound_t_bkgER = #-> your code here <-\n",
    "#-> your code here <-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f97319",
   "metadata": {},
   "source": [
    "## 3.2 Toy-Dataset generation with nuisance parameters\n",
    "\n",
    "To generate toy-datasets we need to random sample also nuisance parameters, which affect the backgrounds expected number of events.\n",
    "\n",
    "So, for both ER and NR background components, we first sample the nuisance parameter values, which we call $t^*_{bkgER}$ and $t^*_{bkgNR}$. Then compute the expectation value $\\mu_{bkg}(t^*_{bkg})$ and generate the number of background events by the usual poissonian sampling based on $\\mu_{bkg}(t^*_{bkg})$.\n",
    "\n",
    "Tip: You want to record the generated $t^*$, so make sure you instantiate global variables accessible later on, outside the toy-dataset generator routine. For sensitivity studies, the gaussian penalty term in the likelihood function for the nuisance parameters will refer to $t^*$ as the nominal value, instead of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d359714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/// Dataset generation with nuisance parameter sampling\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "def generate_toy_dataset_nuis_params(mu_sig, mu_bkgER=mu_bkgER, mu_bkgNR=mu_bkgNR, plot=False):\n",
    "    '''\n",
    "    Returns\n",
    "    dataset : a dataframe of generated events (Toy experiment)\n",
    "    subsets : list of dataframes for signal, background ER and NR\n",
    "    '''\n",
    "    # define global variables for both t_star\n",
    "    #-> your code here <-\n",
    "    \n",
    "    # draw a random value for both t_star from the normal distribution \n",
    "    #-> your code here <-\n",
    "    print('Extracted nuis params:', t_bkgER_star, t_bkgNR_star)\n",
    "    \n",
    "    # compute the background expectation values (mu_bkgER(t_bkgER), mu_bkgNR(t_bkgNR))\n",
    "    #-> your code here <- \n",
    "    print('Expectation values:', mu_sig, mu_bkgER, mu_bkgNR)\n",
    "    \n",
    "    # draw random values for no. of signal and background events\n",
    "    #-> your code here <- \n",
    "\n",
    "    #/// Dataset generation\n",
    "    #-> your code here <- \n",
    "    \n",
    "    if plot:\n",
    "        plot_toy_dataset(subsets, mu_sig)\n",
    "    \n",
    "    return dataset, subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a308f951",
   "metadata": {},
   "source": [
    "## 3.3 Likelihood function $L(\\mu_{sig}; t_{bkgER}, t_{bkgNR})$\n",
    "\n",
    "Extend the definition of the likelihood function to add the dependency on the nuisance parameters.\n",
    "\n",
    "- Background expectation values as a function of nuisance parameters\n",
    "- Add the Gaussian penalty terms for the nuisance parameters that take the form $-\\frac{(t-t^*)^2}{2}$\n",
    "\n",
    "Hence $-2\\log L$ can be expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "-2 \\, \\text{ln} \\,  L(\\mu_{sig};  t_{bkgER}, t_{bkgNR}) & = 2 \\, (\\mu_{sig}+\\mu_{bkgER}(t_{bkgER})+\\mu_{bkgNR}(t_{bkgNR})) -2 \\, \\sum_{i=1}^{N_{obs}} \\big[\\mu_{sig} \\, f_{sig}(S1_i) \\, g_{NR}(Y_i) \\\\\n",
    "                             & \\quad +\\mu_{bkgER}(t_{bkgER}) \\, f_{bkgER}(S1_i) \\, g_{ER}(Y_i) + \\mu_{bkgNR}(t_{bkgNR}) \\, f_{bkgNR}(S1_i) \\, g_{NR}(Y_i)\\big] \\\\\n",
    " & \\quad + (t_{bkgER}-t_{bkgER}^*)^2 + (t_{bkgNR}-t_{bkgNR}^*)^2 \\:,\n",
    "\\end{split}\n",
    "\\label{eq:log-l_LR}\n",
    "\\end{equation}\n",
    "\n",
    "where $t^*$ is the nominal value of the nuisance parameters ($t^*=0$ for hypothesis testing on observed data; $t^*$ given by each different toy-dataset random generation for sensitivity studies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381e33ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/// Unconstrained Profile Likelihood function definition\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "def logProfileLikelihood(parameters, verbose=True):\n",
    "    '''    \n",
    "    -2 * Log-Likelihood function\n",
    "    LL(mu_sig; t_bkgER, t_bkgNR)\n",
    "    '''\n",
    "    #-> your code here <-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b613806e",
   "metadata": {},
   "source": [
    "### 3.3.1 Unconditional and conditional MLE\n",
    "For a given dataset perform both the unconditional and conditional likelihood fit of data. Try out the conditional fit for a given test $\\mu_{sig}$ (e.g. =10).\n",
    "\n",
    "Evaluate the log-likelihood curves in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72a2f05-8794-43f3-af82-8afabb76586d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/// Likelihood fit on an example toy-experiment dataset\n",
    "#/// Compare simple likelihood fit with\n",
    "#/// the unconditional profile likelihood fit\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "#/// Toy-experiment dataset\n",
    "dataset = #-> your code here <-\n",
    "t_bkgER_star = t_bkgNR_star = 0 # reset nuisance parameters central value\n",
    "\n",
    "\n",
    "#/// Simple likelihood (no nuisance parameters) minimization\n",
    "#-> your code here <-\n",
    "\n",
    "#/// Plot log-Likelihood function vs mu_sig\n",
    "#-> your code here <-\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "#/// Unconditional profile likelihood minimization\n",
    "# Maximum likelihood estimators of mu_sig, t_bkgER, t_bkgNR\n",
    "\n",
    "# Tip: you can replicate the minimization with 'minimize', \n",
    "# where parameters will be an array of 3 elements instead of 1 \n",
    "# (and so will be the initial parameter values and bounds)\n",
    "\n",
    "# Tip #2: Use method='SLSQP' in the minimize call\n",
    "\n",
    "# signal strength under test\n",
    "#-> your code here <-\n",
    "fit_res = minimize(logProfileLikelihood, (1,1,1), bounds=[(-20,50),(lowerbound_t_bkgER,5),(lowerbound_t_bkgNR,5)], method='SLSQP')\n",
    "# MLEs\n",
    "# The best-fit parameters will be given as an array:  fit_res['x']\n",
    "mu_sig_hat  = #-> your code here <-\n",
    "t_bkgER_hat = #-> your code here <-\n",
    "t_bkgNR_hat = #-> your code here <-\n",
    "\n",
    "#/// Plot log-Likelihood function vs mu_sig\n",
    "#-> your code here <-\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "#/// Conditional profile likelihood minimization\n",
    "# Maximum likelihood estimators of t_bkgER, t_bkgNR, given a fixed mu_sig\n",
    "\n",
    "# Tip: you can replicate the minimization with 'minimize', \n",
    "# and impose bounds to mu_sig so that it is fixed to a single value\n",
    "\n",
    "# Tip #2: Use method='Powell' in this case\n",
    "\n",
    "# signal strength under test (and fixed in the likelihood fit)\n",
    "#-> your code here <-\n",
    "\n",
    "# Conditional likelihood fit\n",
    "fit_res = minimize(logProfileLikelihood, (mu_sig,1,1), \n",
    "                   bounds=[(mu_sig,mu_sig),(lowerbound_t_bkgER,5),(lowerbound_t_bkgNR,5)], \n",
    "                   method='Powell')\n",
    "# MLEs\n",
    "t_bkgER_doublehat = #-> your code here <-\n",
    "t_bkgNR_doublehat = #-> your code here <-\n",
    "\n",
    "#/// Plot log-Likelihood function (with fixed mu_sig) vs t_bkgER and t_bkgNR\n",
    "#-> your code here <-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a00ff72",
   "metadata": {},
   "source": [
    "## 3.4 Test statistic: Profile Likelihood Ratio\n",
    "Write the profile likelihood ratio test statistic.\n",
    "\n",
    "Tip: write a routine that, given $\\mu_{sig}$, computes $q_{\\mu}$ on a dataset, and that performs the conditional likelihood fit only when needed to optimize speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2fe17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/// Profile likelihood ratio test statistic (for signal hypotesis rejection)\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "def q_mu_PLR(mu_sig):\n",
    "    '''\n",
    "    Returns the Profile Likelihood Ratio test statistic\n",
    "    for signal hypothesis H_mu rejection\n",
    "    ---    \n",
    "    mu_sig : the tested signal strength\n",
    "    '''\n",
    "    # Tip: in this case you can try to implement inside this routine\n",
    "    # also the likelihood minimizations required to compute the test statistic value\n",
    "    # on a given dataset and for a tested mu_sig\n",
    "    # (so that you optimize the likelihood minimizations required - they are computationally expensive!)\n",
    "    \n",
    "    # Compute the unconditional MLE of mu_sig\n",
    "    #-> your code here <-\n",
    "    \n",
    "    #/// Test statistic value, case 1\n",
    "    if mu_sig_hat > mu_sig:\n",
    "        q = #-> your code here <-\n",
    "        \n",
    "    else:\n",
    "        # Compute the conditional maximum likelihood\n",
    "        #-> your code here <-\n",
    "        \n",
    "        #/// Test statistic value, case 2\n",
    "        q = #-> your code here <-\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f134f4",
   "metadata": {},
   "source": [
    "## 3.4.1 Test statistic PDF under $H_\\mu$\n",
    "\n",
    "You can retrieve already produced test statistic distributions $f(q_\\mu|H_\\mu)$ from the same pickle file `test_stat_distributions.pkl` for some $\\mu_{sig}$.\n",
    "\n",
    "Available test signal strengths are: `mu_test = [0.1,5,13,19]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ee193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/// PLR Test statistic distribution under a given signal hypothesis: mu_sig\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "def qmuPLR_Hmu_distr(mu_sig, ntoys=1000, write2pickle=True):\n",
    "    '''\n",
    "    Compute f(q_mu|H_mu) with toy-MC (many toy-datasets)\n",
    "    Test static distribution under signal hypothesis H_mu\n",
    "    For a given signal strength mu_sig tested\n",
    "    \n",
    "    mu_sig : test signal strength\n",
    "    ntoys  : no. of toy-experiment datasets to be simulated\n",
    "    \n",
    "    write2pickle : save q values in a dataframe and write to pickle file\n",
    "    \n",
    "    Returns array of test statistic values\n",
    "    '''\n",
    "    #-> your code here <-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5f97c",
   "metadata": {},
   "source": [
    "## 3.5 Exclusion limit at 90% CL\n",
    "\n",
    "Compute the 90% CL exclusion limit on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217edba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/// Compute exclusion limit for a single toy-experiment with the PLR method\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "def exclusion_limit_PLR(data, cl=0.9):\n",
    "    \n",
    "    # Dataset to analyze (instantiate as global variable, used by the likelihood function)\n",
    "    #-> your code here <-\n",
    "\n",
    "    #/// Plot dataset in (S1,Y) space\n",
    "    #-> your code here <-\n",
    "    \n",
    "    # set t_star=0 for limit calculation (not sensitivity study)\n",
    "    #-> your code here <-\n",
    "    \n",
    "    # Read test statistic distribution f(qmu|Hmu)\n",
    "    #-> your code here <-\n",
    "    \n",
    "    # array of tested mu_sig\n",
    "    mu_test = [0.1,5,13,19]\n",
    "       \n",
    "    #/// Compute p-value for H_mu rejection for different signal strengths mu_sig\n",
    "    #-> your code here <-\n",
    "\n",
    "    #/// Plot p-value vs mu_sig\n",
    "    #-> your code here <-\n",
    "\n",
    "    #/// Compute exclusion limit a 90% CL\n",
    "    #-> your code here <-\n",
    "    \n",
    "    # show limit value\n",
    "    #-> your code here <-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c7c7f0",
   "metadata": {},
   "source": [
    "# 4. SENSITIVITY WITH THE LIKELIHOOD RATIO METHOD\n",
    "The sensitivity gives you the exclusion limit expected based on your experimental physical model. More precisely, to study the sensitivity you estimate the distribution of the expected exclusion limits (under the null hypothesis, no signal). Instead of computing the exclusion limit on a given dataset, now you want to simulate many datasets under the null hypothesis $H_0$ and estimate $f(q_\\mu|H_0)$ for signal strengths $\\mu_{sig}$ in the range of interest.\n",
    "\n",
    "The median (1-2$\\sigma$) sensitivity corresponds to the observed test statistic $q_{\\mu}$ given by the median (1-2$\\sigma$) quantile of $f(q_\\mu|H_0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d670c41",
   "metadata": {},
   "source": [
    "## 4.1 Expected test statistic PDF under null hypothesis: $f(q_\\mu|H_0)$\n",
    "- Generate ~$10^4$ toy-MCs under the background-only hypothesis $H_0$ \n",
    "- Compute $q_\\mu$ on each toy-MC for a set of signal strengths $\\mu_{sig}$ and build $f(q_\\mu|H_0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a65fea",
   "metadata": {},
   "source": [
    "For a given $\\mu_{sig}$ plot $f(q_\\mu|H_0)$ vs $f(q_\\mu|H_\\mu)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a8e4a3",
   "metadata": {},
   "source": [
    "## 4.2 Median sensitivity\n",
    "- Based on the median values of $f(q_\\mu|H_0)$ for each $\\mu$, find the limit at 90% CL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dc1f5a",
   "metadata": {},
   "source": [
    "## 4.3 1 and 2 sigma bands\n",
    "- Find the 90% CL limit corresponding to the $\\pm1$ and $\\pm2\\sigma$ quantiles of $f(q_\\mu|H_0)$ to find the 1 and 2$\\sigma$ bands of the sensitivity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e3d422",
   "metadata": {},
   "source": [
    "## 4.4 Exclusion limit outcome vs expected sensitivity\n",
    "- Compare the exclusion limit obtained previously with the given dataset with the expected sensitivity (median and 1/2 sigma bands)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01d2c70",
   "metadata": {},
   "source": [
    "### 4.4.1 CL$_s$ or Power constraint\n",
    "Underfluctuations of background in the observed dataset can lead to strong exclusion limits on values of the PoI to which the experiment is poorly sensitive. It can even happen that your exclusion limit is zero(!). \n",
    "\n",
    "One solution is the **CLs method**:\n",
    "Use a modified p-value\n",
    "\n",
    "$p' = \\frac{p}{1-p_b}$, where\n",
    "\n",
    "$1 - p_b = \\int^\\inf_{q_{\\mu}^{obs}}f(q_\\mu|H_0) dq_\\mu$\n",
    "\n",
    "This method gives weaker limits but provides overcoverage, so it is conservative.\n",
    "\n",
    "Another approach is the **Power constraint**, in which an experiment claims the $-1\\sigma$ sensitivity as limit in case the actual limit is below that level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6762b208",
   "metadata": {},
   "source": [
    "# 5 Discovery significance and potential\n",
    "In this case you want to test and reject the null hypothesis (instead of the signal hypotheses, as we did before to set exclusion limits). \n",
    "\n",
    "## 5.1 Test statistic for discovery\n",
    "You can evaluate the significance of the rejection of the background-only hypothesis (which implies discovery!) by means of the **profile likelihood ratio test statistic for discovery** $q_0$:\n",
    "\n",
    "\\begin{equation}\n",
    "q_0=\n",
    "\\begin{cases}\n",
    "-2 \\, \\log \\, \\frac{L(0 , \\, \\hat{\\hat{\\boldsymbol{\\theta}}}(0))}{L(\\hat{\\mu} , \\, \\hat{\\boldsymbol{\\theta}})} & \\hat{\\mu} \\geq 0\\\\[1.5ex]\n",
    "0 & \\hat{\\mu} < 0 \\: ,\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "## 5.2 p-value (discovery significance)\n",
    "If you evaluate $q_0$ on a given dataset and compare it with its PDF under the null hypothesis $H_0$ ($f(q_0|H_0)$, you can calculate the **p-value** exactly as you did before in the case of $H_\\mu$ rejection.\n",
    "\n",
    "Then the p-value is conventionally translated into significance $Z$, considering a one-sided normal distribution. If you consider the p-value as the one-sided integral of a normal distribution, $I=\\int_{-\\infty}^{Z}N(x)dx$, you have the 1:1 correspondence between p-value and $Z$ (p-value = $1-I$). Hence, e.g. $3\\sigma$ significance corresponds to $Z=3$ -> $I=0.9987$ -> p-value = $1.3\\times10^{-3}$.\n",
    "\n",
    "\n",
    "## 5.3 Discovery potential\n",
    "The expected discovery limit at $3\\sigma$ significance (e.g.) is the minimum WIMP cross section (or the smallest signal strength $\\mu_{sig}$) for which the null hypothesis $H_0$ is rejected with a significance of at least $3\\sigma$. To estimate the expected discovery potential of an experiment, one has to simulate many toy-datasets and estimate the test statistic distribution under signal hypotheses $f(q_0|H_\\mu)$ for a sensible range of $\\mu_{sig}$. Then for each $\\mu_{sig}$ you can consider the median of $f(q_0|H_\\mu)$ and compute the p-value for $H_0$ rejection.\n",
    "\n",
    "In the case of discovery one usually considers the 90% or 95% quantile of $f(q_0|H_\\mu)$, instead of the median. That means the experimental discovery limit is xxx (what you find) with 90% probability (instead of with 50% probability, as in the case you consider the median).\n",
    "\n",
    "As for the discovery significance one usually also considers $5\\sigma$, besides $3\\sigma$ (just conventions anyway...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ae399d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
